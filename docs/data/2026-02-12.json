{
  "date": "2026-02-12",
  "meta": {
    "generatedAt": "2026-02-12T20:22:24.444229+08:00",
    "tz": "Asia/Shanghai",
    "freshness": "pd",
    "queriesRun": 20,
    "subreddits": [
      "LocalLLaMA",
      "MachineLearning",
      "OpenAI",
      "AI_Agents"
    ]
  },
  "items": [
    {
      "title": "Meta launches AI algorithm personalization feature for Threads",
      "url": "https://www.cnbc.com/2026/02/11/meta-threads-dear-algo-ai-algorithm-personalization.html",
      "description": "In this photo illustration, the ... on September 4, 2024. ... <strong>Meta on Wednesday debuted an AI feature called &quot;Dear Algo&quot; that lets Threads users personalize their content-recommendation algorithms</strong>....",
      "host": "www.cnbc.com",
      "published": null,
      "category": "产品发布/模型更新",
      "score": 3.8
    },
    {
      "title": "Ray-Ban maker EssilorLuxottica says it more than tripled Meta AI glasses sales in 2025",
      "url": "https://www.cnbc.com/2026/02/11/ray-ban-maker-essilorluxottica-triples-sales-of-meta-ai-glasses.html",
      "description": "The two companies launched the first edition of the glasses in <strong>September 2021</strong>, but the device didn&#x27;t gain widespread attention until the second-generation launch in 2023. In September, EssilorLuxottica and Meta introduced a new Ray-Ban iteration, ...",
      "host": "www.cnbc.com",
      "published": null,
      "category": "产品发布/模型更新",
      "score": 3.55
    },
    {
      "title": "[2602.10386] Colorful Talks with Graphs: Human-Interpretable Graph Encodings for Large Language Models",
      "url": "https://arxiv.org/abs/2602.10386",
      "description": "By capturing both local and global-range dependencies, our method enhances LLM performance especially on graph tasks that require reasoning over global graph structure. From: Angelo Zangari [view email] [v1] Wed, 11 Feb 2026 00:15:29 UTC (218 KB) ... View a PDF of the paper titled Colorful Talks with Graphs: Human-Interpretable Graph Encodings for Large Language Models, by Angelo Zangari and 2 other authors ... arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.",
      "host": "arxiv.org",
      "published": null,
      "category": "研究/论文",
      "score": 0.4
    },
    {
      "title": "[2602.10485] Abstraction Generation for Generalized Planning with Pretrained Large Language Models",
      "url": "https://arxiv.org/abs/2602.10485",
      "description": "Recent works show that large language models (LLMs) can function as generalized planners. This work investigates whether LLMs can serve as QNP abstraction generators for GP problems and how to fix abstractions via automated debugging. We propose a prompt protocol: input a GP domain and training tasks to LLMs, prompting them to generate abstract features and further abstract the initial state, action set, and goal into QNP problems. An automated debugging method is designed to detect abstraction errors, guiding LLMs to fix abstractions.",
      "host": "arxiv.org",
      "published": null,
      "category": "研究/论文",
      "score": 0.4
    },
    {
      "title": "[2602.11091] Can Large Language Models Make Everyone Happy?",
      "url": "https://arxiv.org/abs/2602.11091",
      "description": "View a PDF of the paper titled Can Large Language Models Make Everyone Happy?, by Usman Naseem and 5 other authors ... arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.",
      "host": "arxiv.org",
      "published": null,
      "category": "研究/论文",
      "score": 0.4
    },
    {
      "title": "Search or Accelerate: Confidence-Switched Position Beam Search for Diffusion Language Models",
      "url": "https://arxiv.org/html/2602.10953",
      "description": "We introduce <strong>Search Or AcceleRate (SOAR),</strong> an adaptive inference framework that dynamically balances generation quality and speed for Diffusion Language Models. By allowing the model to widen its search when uncertain and accelerate when confident, SOAR consistently improves generation quality ...",
      "host": "arxiv.org",
      "published": null,
      "category": "研究/论文",
      "score": 0.4
    },
    {
      "title": "Can Large Language Models Make Everyone Happy?",
      "url": "https://arxiv.org/html/2602.11091",
      "description": "Extensive experiments across diverse model families demonstrate that misalignment emerges from consistent internal conflicts rather than isolated failures, providing new insights for alignment research and evaluation. Despite its scale and scope, MisAlign-Profile is limited to English-language prompts and focuses primarily on normative domains derived from existing taxonomies.",
      "host": "arxiv.org",
      "published": null,
      "category": "研究/论文",
      "score": 0.4
    },
    {
      "title": "[2602.10874] C-MOP: Integrating Momentum and Boundary-Aware Clustering for Enhanced Prompt Evolution",
      "url": "https://arxiv.org/abs/2602.10874",
      "description": "<strong>Automatic prompt optimization</strong> is a promising direction to boost the performance of Large Language Models (LLMs). However, existing methods often suffer from noisy and conflicting update signals.",
      "host": "arxiv.org",
      "published": null,
      "category": "研究/论文",
      "score": 0.4
    }
  ],
  "reddit": [
    {
      "subreddit": "LocalLLaMA",
      "title": "Announcing LocalLlama discord server &amp; bot!",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/",
      "score": 123,
      "comments": 66
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "Z.ai said they are GPU starved, openly.",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r26zsg/zai_said_they_are_gpu_starved_openly/",
      "score": 1220,
      "comments": 199
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "#SaveLocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r2e8mp/savelocalllama/",
      "score": 512,
      "comments": 64
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "Unsloth just unleashed Glm 5! GGUF NOW!",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r2i4lw/unsloth_just_unleashed_glm_5_gguf_now/",
      "score": 174,
      "comments": 48
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "GLM-5 scores 50 on the Intelligence Index and is the new open weights leader!",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r28xxz/glm5_scores_50_on_the_intelligence_index_and_is/",
      "score": 511,
      "comments": 121
    },
    {
      "subreddit": "LocalLLaMA",
      "title": "Lobotomy-less REAP by Samsung (REAM)",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r2moge/lobotomyless_reap_by_samsung_ream/",
      "score": 69,
      "comments": 27
    },
    {
      "subreddit": "MachineLearning",
      "title": "[D] Self-Promotion Thread",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qtjnbc/d_selfpromotion_thread/",
      "score": 6,
      "comments": 23
    },
    {
      "subreddit": "MachineLearning",
      "title": "[D] Monthly Who's Hiring and Who wants to be Hired?",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qrrayn/d_monthly_whos_hiring_and_who_wants_to_be_hired/",
      "score": 14,
      "comments": 7
    },
    {
      "subreddit": "MachineLearning",
      "title": "[D] Is a KDD publication considered prestigious for more theoretical results?",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1r2l6w4/d_is_a_kdd_publication_considered_prestigious_for/",
      "score": 15,
      "comments": 17
    },
    {
      "subreddit": "MachineLearning",
      "title": "[D] CVPR Score stats",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1r2pqeg/d_cvpr_score_stats/",
      "score": 2,
      "comments": 1
    },
    {
      "subreddit": "MachineLearning",
      "title": "[R] ICLR: Guess which peer review is human or AI?",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1r28sy7/r_iclr_guess_which_peer_review_is_human_or_ai/",
      "score": 25,
      "comments": 20
    },
    {
      "subreddit": "MachineLearning",
      "title": "[P] Graph Representation Learning Help",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1r2gpz6/p_graph_representation_learning_help/",
      "score": 5,
      "comments": 4
    },
    {
      "subreddit": "OpenAI",
      "title": "Sora 2 megathread (part 3)",
      "url": "https://www.reddit.com/r/OpenAI/comments/1o8kmg9/sora_2_megathread_part_3/",
      "score": 302,
      "comments": 9720
    },
    {
      "subreddit": "OpenAI",
      "title": "AMA on our DevDay Launches",
      "url": "https://www.reddit.com/r/OpenAI/comments/1o1j23g/ama_on_our_devday_launches/",
      "score": 112,
      "comments": 528
    },
    {
      "subreddit": "OpenAI",
      "title": "In the past week alone:",
      "url": "https://www.reddit.com/r/OpenAI/comments/1r25bh7/in_the_past_week_alone/",
      "score": 3357,
      "comments": 459
    },
    {
      "subreddit": "OpenAI",
      "title": "'QuitGPT' Campaign Wants You to Ditch ChatGPT Over OpenAI's Ties to Trump, ICE",
      "url": "https://www.reddit.com/r/OpenAI/comments/1r2n41e/quitgpt_campaign_wants_you_to_ditch_chatgpt_over/",
      "score": 170,
      "comments": 99
    },
    {
      "subreddit": "OpenAI",
      "title": "How is this not the biggest news right now?",
      "url": "https://www.reddit.com/r/OpenAI/comments/1r2jdg4/how_is_this_not_the_biggest_news_right_now/",
      "score": 247,
      "comments": 34
    },
    {
      "subreddit": "OpenAI",
      "title": "Oh my God, the update that they did at 5.2 is absolutely insane.",
      "url": "https://www.reddit.com/r/OpenAI/comments/1r221yu/oh_my_god_the_update_that_they_did_at_52_is/",
      "score": 855,
      "comments": 473
    },
    {
      "subreddit": "AI_Agents",
      "title": "Weekly Thread: Project Display",
      "url": "https://www.reddit.com/r/AI_Agents/comments/1r22uuw/weekly_thread_project_display/",
      "score": 1,
      "comments": 10
    },
    {
      "subreddit": "AI_Agents",
      "title": "Weekly Thread: Project Display",
      "url": "https://www.reddit.com/r/AI_Agents/comments/1qvu2c4/weekly_thread_project_display/",
      "score": 3,
      "comments": 23
    },
    {
      "subreddit": "AI_Agents",
      "title": "What are some typical, basic agentic usecases?",
      "url": "https://www.reddit.com/r/AI_Agents/comments/1r2pcgb/what_are_some_typical_basic_agentic_usecases/",
      "score": 11,
      "comments": 9
    },
    {
      "subreddit": "AI_Agents",
      "title": "Vibecoding might be one of the most underrated skill tests today",
      "url": "https://www.reddit.com/r/AI_Agents/comments/1r2mdpu/vibecoding_might_be_one_of_the_most_underrated/",
      "score": 10,
      "comments": 16
    },
    {
      "subreddit": "AI_Agents",
      "title": "Is agentic scraping actually the endgame?",
      "url": "https://www.reddit.com/r/AI_Agents/comments/1r2rf2r/is_agentic_scraping_actually_the_endgame/",
      "score": 3,
      "comments": 4
    },
    {
      "subreddit": "AI_Agents",
      "title": "hitting limits with our AI driven web automation",
      "url": "https://www.reddit.com/r/AI_Agents/comments/1r2ng93/hitting_limits_with_our_ai_driven_web_automation/",
      "score": 5,
      "comments": 11
    }
  ]
}