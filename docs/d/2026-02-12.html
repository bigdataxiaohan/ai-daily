<!doctype html><html><head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1' />
<link rel='stylesheet' href='./assets/style.css' />
<title>AI 日报 | 2026-02-12</title></head><body>
<div class='wrap'>
<div class='hero'>
<div class='h1'>AI 日报 + 情报监控</div>
<div class='sub'>日期：2026-02-12（北京时间）<br/>来源：Brave Search（headline+snippet） + Reddit（best-effort）</div>
<div class='chips'>
<span class='chip'>总条目 7</span>
<span class='chip'>Reddit 条目 24</span>
<span class='chip'>freshness=pd</span>
</div>
</div>
<div class='section'>
<div class='section-title'>今日重点（Top）</div>
<div class='grid'>
<div class='card'><div class='row'><div style='display:flex;gap:8px;flex-wrap:wrap'><span class='pill' style='background:#1976D2'>热度 ★★★</span><span class='pill' style='background:#F57C00'>产品发布/模型更新</span></div></div><div class='title'>01. <a href='https://www.cnbc.com/2026/02/11/ray-ban-maker-essilorluxottica-triples-sales-of-meta-ai-glasses.html'>Ray-Ban maker EssilorLuxottica says it more than tripled Meta AI glasses sales in 2025</a></div><div class='desc'>The two companies launched the first edition of the glasses in &lt;strong&gt;September 2021&lt;/strong&gt;, but the device didn&amp;#x27;t gain widespread attention until the second-generation launch in 2023. In Sep…</div><div style='margin-top:10px'><a class='btn' href='https://www.cnbc.com/2026/02/11/ray-ban-maker-essilorluxottica-triples-sales-of-meta-ai-glasses.html'>打开链接</a></div></div>
<div class='card'><div class='row'><div style='display:flex;gap:8px;flex-wrap:wrap'><span class='pill' style='background:#607D8B'>热度 ★</span><span class='pill' style='background:#455A64'>研究/论文</span></div></div><div class='title'>02. <a href='https://arxiv.org/abs/2602.10418'>[2602.10418] SecCodePRM: A Process Reward Model for Code Security</a></div><div class='desc'>View a PDF of the paper titled SecCodePRM: A Process Reward Model for Code Security, by Weichen Yu and 6 other authors View PDF HTML (experimental) Abstract:Large Language Models are rapidly becoming…</div><div style='margin-top:10px'><a class='btn' href='https://arxiv.org/abs/2602.10418'>打开链接</a></div></div>
<div class='card'><div class='row'><div style='display:flex;gap:8px;flex-wrap:wrap'><span class='pill' style='background:#607D8B'>热度 ★</span><span class='pill' style='background:#455A64'>研究/论文</span></div></div><div class='title'>03. <a href='https://arxiv.org/abs/2602.10386'>[2602.10386] Colorful Talks with Graphs: Human-Interpretable Graph Encodings for Large Language Models</a></div><div class='desc'>By capturing both local and global-range dependencies, our method enhances LLM performance especially on graph tasks that require reasoning over global graph structure. From: Angelo Zangari [view ema…</div><div style='margin-top:10px'><a class='btn' href='https://arxiv.org/abs/2602.10386'>打开链接</a></div></div>
<div class='card'><div class='row'><div style='display:flex;gap:8px;flex-wrap:wrap'><span class='pill' style='background:#607D8B'>热度 ★</span><span class='pill' style='background:#455A64'>研究/论文</span></div></div><div class='title'>04. <a href='https://arxiv.org/abs/2602.10485'>[2602.10485] Abstraction Generation for Generalized Planning with Pretrained Large Language Models</a></div><div class='desc'>Recent works show that large language models (LLMs) can function as generalized planners. This work investigates whether LLMs can serve as QNP abstraction generators for GP problems and how to fix ab…</div><div style='margin-top:10px'><a class='btn' href='https://arxiv.org/abs/2602.10485'>打开链接</a></div></div>
<div class='card'><div class='row'><div style='display:flex;gap:8px;flex-wrap:wrap'><span class='pill' style='background:#607D8B'>热度 ★</span><span class='pill' style='background:#455A64'>研究/论文</span></div></div><div class='title'>05. <a href='https://arxiv.org/abs/2601.21204'>[2601.21204] Scaling Embeddings Outperforms Scaling Experts in Language Models</a></div><div class='desc'>View a PDF of the paper titled &lt;strong&gt;Scaling Embeddings Outperforms Scaling Experts in Language Models, by Hong Liu and 15 other authors View PDF HTML (experimental) Abstract:While Mixture-of-Exper…</div><div style='margin-top:10px'><a class='btn' href='https://arxiv.org/abs/2601.21204'>打开链接</a></div></div>
<div class='card'><div class='row'><div style='display:flex;gap:8px;flex-wrap:wrap'><span class='pill' style='background:#607D8B'>热度 ★</span><span class='pill' style='background:#455A64'>研究/论文</span></div></div><div class='title'>06. <a href='https://arxiv.org/abs/2602.10382'>[2602.10382] Triggers Hijack Language Circuits: A Mechanistic Analysis of Backdoor Behaviors in Large Language Models</a></div><div class='desc'>From: Théo Lasnier [view email] [v1] Wed, 11 Feb 2026 00:04:32 UTC (197 KB) ... View a PDF of the paper titled &lt;strong&gt;Triggers Hijack Language Circuits&lt;/strong&gt;: A Mechanistic Analysis of Backdoor B…</div><div style='margin-top:10px'><a class='btn' href='https://arxiv.org/abs/2602.10382'>打开链接</a></div></div>
<div class='card'><div class='row'><div style='display:flex;gap:8px;flex-wrap:wrap'><span class='pill' style='background:#607D8B'>热度 ★</span><span class='pill' style='background:#455A64'>研究/论文</span></div></div><div class='title'>07. <a href='https://arxiv.org/abs/2602.10467'>[2602.10467] MERIT Feedback Elicits Better Bargaining in LLM Negotiators</a></div><div class='desc'>Abstract:Bargaining is often regarded ... yet Large Language Models (LLMs) still struggle to navigate it due to limited strategic depth and difficulty adapting to complex human factors. Current bench…</div><div style='margin-top:10px'><a class='btn' href='https://arxiv.org/abs/2602.10467'>打开链接</a></div></div>
</div>
</div>
<div class='section'>
<div class='section-title'>产品发布/模型更新（1）</div>
<div class='grid'>
<div class='card'><div class='row'><div style='display:flex;gap:8px;flex-wrap:wrap'><span class='pill' style='background:#1976D2'>热度 ★★★</span><span class='pill' style='background:#F57C00'>产品发布/模型更新</span></div></div><div class='title'>01. <a href='https://www.cnbc.com/2026/02/11/ray-ban-maker-essilorluxottica-triples-sales-of-meta-ai-glasses.html'>Ray-Ban maker EssilorLuxottica says it more than tripled Meta AI glasses sales in 2025</a></div><div class='desc'>The two companies launched the first edition of the glasses in &lt;strong&gt;September 2021&lt;/strong&gt;, but the device didn&amp;#x27;t gain widespread attention until the second-generation launch in 2023. In Sep…</div><div style='margin-top:10px'><a class='btn' href='https://www.cnbc.com/2026/02/11/ray-ban-maker-essilorluxottica-triples-sales-of-meta-ai-glasses.html'>打开链接</a></div></div>
</div>
</div>
<div class='section'>
<div class='section-title'>研究/论文（6）</div>
<div class='grid'>
<div class='card'><div class='row'><div style='display:flex;gap:8px;flex-wrap:wrap'><span class='pill' style='background:#607D8B'>热度 ★</span><span class='pill' style='background:#455A64'>研究/论文</span></div></div><div class='title'>01. <a href='https://arxiv.org/abs/2602.10418'>[2602.10418] SecCodePRM: A Process Reward Model for Code Security</a></div><div class='desc'>View a PDF of the paper titled SecCodePRM: A Process Reward Model for Code Security, by Weichen Yu and 6 other authors View PDF HTML (experimental) Abstract:Large Language Models are rapidly becoming…</div><div style='margin-top:10px'><a class='btn' href='https://arxiv.org/abs/2602.10418'>打开链接</a></div></div>
<div class='card'><div class='row'><div style='display:flex;gap:8px;flex-wrap:wrap'><span class='pill' style='background:#607D8B'>热度 ★</span><span class='pill' style='background:#455A64'>研究/论文</span></div></div><div class='title'>02. <a href='https://arxiv.org/abs/2602.10386'>[2602.10386] Colorful Talks with Graphs: Human-Interpretable Graph Encodings for Large Language Models</a></div><div class='desc'>By capturing both local and global-range dependencies, our method enhances LLM performance especially on graph tasks that require reasoning over global graph structure. From: Angelo Zangari [view ema…</div><div style='margin-top:10px'><a class='btn' href='https://arxiv.org/abs/2602.10386'>打开链接</a></div></div>
<div class='card'><div class='row'><div style='display:flex;gap:8px;flex-wrap:wrap'><span class='pill' style='background:#607D8B'>热度 ★</span><span class='pill' style='background:#455A64'>研究/论文</span></div></div><div class='title'>03. <a href='https://arxiv.org/abs/2602.10485'>[2602.10485] Abstraction Generation for Generalized Planning with Pretrained Large Language Models</a></div><div class='desc'>Recent works show that large language models (LLMs) can function as generalized planners. This work investigates whether LLMs can serve as QNP abstraction generators for GP problems and how to fix ab…</div><div style='margin-top:10px'><a class='btn' href='https://arxiv.org/abs/2602.10485'>打开链接</a></div></div>
<div class='card'><div class='row'><div style='display:flex;gap:8px;flex-wrap:wrap'><span class='pill' style='background:#607D8B'>热度 ★</span><span class='pill' style='background:#455A64'>研究/论文</span></div></div><div class='title'>04. <a href='https://arxiv.org/abs/2601.21204'>[2601.21204] Scaling Embeddings Outperforms Scaling Experts in Language Models</a></div><div class='desc'>View a PDF of the paper titled &lt;strong&gt;Scaling Embeddings Outperforms Scaling Experts in Language Models, by Hong Liu and 15 other authors View PDF HTML (experimental) Abstract:While Mixture-of-Exper…</div><div style='margin-top:10px'><a class='btn' href='https://arxiv.org/abs/2601.21204'>打开链接</a></div></div>
<div class='card'><div class='row'><div style='display:flex;gap:8px;flex-wrap:wrap'><span class='pill' style='background:#607D8B'>热度 ★</span><span class='pill' style='background:#455A64'>研究/论文</span></div></div><div class='title'>05. <a href='https://arxiv.org/abs/2602.10382'>[2602.10382] Triggers Hijack Language Circuits: A Mechanistic Analysis of Backdoor Behaviors in Large Language Models</a></div><div class='desc'>From: Théo Lasnier [view email] [v1] Wed, 11 Feb 2026 00:04:32 UTC (197 KB) ... View a PDF of the paper titled &lt;strong&gt;Triggers Hijack Language Circuits&lt;/strong&gt;: A Mechanistic Analysis of Backdoor B…</div><div style='margin-top:10px'><a class='btn' href='https://arxiv.org/abs/2602.10382'>打开链接</a></div></div>
<div class='card'><div class='row'><div style='display:flex;gap:8px;flex-wrap:wrap'><span class='pill' style='background:#607D8B'>热度 ★</span><span class='pill' style='background:#455A64'>研究/论文</span></div></div><div class='title'>06. <a href='https://arxiv.org/abs/2602.10467'>[2602.10467] MERIT Feedback Elicits Better Bargaining in LLM Negotiators</a></div><div class='desc'>Abstract:Bargaining is often regarded ... yet Large Language Models (LLMs) still struggle to navigate it due to limited strategic depth and difficulty adapting to complex human factors. Current bench…</div><div style='margin-top:10px'><a class='btn' href='https://arxiv.org/abs/2602.10467'>打开链接</a></div></div>
</div>
</div>
<div class='section'>
<div class='section-title'>Reddit 热门</div>
<div class='grid'>
<div class='card'><div class='row'><div style='display:flex;gap:8px;flex-wrap:wrap'><span class='pill' style='background:#111827'>Reddit</span><span class='pill' style='background:#374151'>r/LocalLLaMA</span></div></div><div class='title'>01. <a href='https://www.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/'>Announcing LocalLlama discord server &amp;amp; bot!</a></div><div class='meta'>score=119 · comments=66</div><div style='margin-top:10px'><a class='btn' href='https://www.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/'>打开链接</a></div></div>
<div class='card'><div class='row'><div style='display:flex;gap:8px;flex-wrap:wrap'><span class='pill' style='background:#111827'>Reddit</span><span class='pill' style='background:#374151'>r/LocalLLaMA</span></div></div><div class='title'>02. <a href='https://www.reddit.com/r/LocalLLaMA/comments/1r26zsg/zai_said_they_are_gpu_starved_openly/'>Z.ai said they are GPU starved, openly.</a></div><div class='meta'>score=997 · comments=170</div><div style='margin-top:10px'><a class='btn' href='https://www.reddit.com/r/LocalLLaMA/comments/1r26zsg/zai_said_they_are_gpu_starved_openly/'>打开链接</a></div></div>
<div class='card'><div class='row'><div style='display:flex;gap:8px;flex-wrap:wrap'><span class='pill' style='background:#111827'>Reddit</span><span class='pill' style='background:#374151'>r/LocalLLaMA</span></div></div><div class='title'>03. <a href='https://www.reddit.com/r/LocalLLaMA/comments/1r2e8mp/savelocalllama/'>#SaveLocalLLaMA</a></div><div class='meta'>score=263 · comments=47</div><div style='margin-top:10px'><a class='btn' href='https://www.reddit.com/r/LocalLLaMA/comments/1r2e8mp/savelocalllama/'>打开链接</a></div></div>
<div class='card'><div class='row'><div style='display:flex;gap:8px;flex-wrap:wrap'><span class='pill' style='background:#111827'>Reddit</span><span class='pill' style='background:#374151'>r/LocalLLaMA</span></div></div><div class='title'>04. <a href='https://www.reddit.com/r/LocalLLaMA/comments/1r28xxz/glm5_scores_50_on_the_intelligence_index_and_is/'>GLM-5 scores 50 on the Intelligence Index and is the new open weights leader!</a></div><div class='meta'>score=356 · comments=84</div><div style='margin-top:10px'><a class='btn' href='https://www.reddit.com/r/LocalLLaMA/comments/1r28xxz/glm5_scores_50_on_the_intelligence_index_and_is/'>打开链接</a></div></div>
<div class='card'><div class='row'><div style='display:flex;gap:8px;flex-wrap:wrap'><span class='pill' style='background:#111827'>Reddit</span><span class='pill' style='background:#374151'>r/LocalLLaMA</span></div></div><div class='title'>05. <a href='https://www.reddit.com/r/LocalLLaMA/comments/1r22hlq/glm5_officially_released/'>GLM-5 Officially Released</a></div><div class='meta'>score=648 · comments=132</div><div style='margin-top:10px'><a class='btn' href='https://www.reddit.com/r/LocalLLaMA/comments/1r22hlq/glm5_officially_released/'>打开链接</a></div></div>
<div class='card'><div class='row'><div style='display:flex;gap:8px;flex-wrap:wrap'><span class='pill' style='background:#111827'>Reddit</span><span class='pill' style='background:#374151'>r/LocalLLaMA</span></div></div><div class='title'>06. <a href='https://www.reddit.com/r/LocalLLaMA/comments/1r2i4lw/unsloth_just_unleashed_glm_5_gguf_now/'>Unsloth just unleashed Glm 5! GGUF NOW!</a></div><div class='meta'>score=59 · comments=21</div><div style='margin-top:10px'><a class='btn' href='https://www.reddit.com/r/LocalLLaMA/comments/1r2i4lw/unsloth_just_unleashed_glm_5_gguf_now/'>打开链接</a></div></div>
<div class='card'><div class='row'><div style='display:flex;gap:8px;flex-wrap:wrap'><span class='pill' style='background:#111827'>Reddit</span><span class='pill' style='background:#374151'>r/MachineLearning</span></div></div><div class='title'>07. <a href='https://www.reddit.com/r/MachineLearning/comments/1qtjnbc/d_selfpromotion_thread/'>[D] Self-Promotion Thread</a></div><div class='meta'>score=5 · comments=22</div><div style='margin-top:10px'><a class='btn' href='https://www.reddit.com/r/MachineLearning/comments/1qtjnbc/d_selfpromotion_thread/'>打开链接</a></div></div>
<div class='card'><div class='row'><div style='display:flex;gap:8px;flex-wrap:wrap'><span class='pill' style='background:#111827'>Reddit</span><span class='pill' style='background:#374151'>r/MachineLearning</span></div></div><div class='title'>08. <a href='https://www.reddit.com/r/MachineLearning/comments/1qrrayn/d_monthly_whos_hiring_and_who_wants_to_be_hired/'>[D] Monthly Who&#x27;s Hiring and Who wants to be Hired?</a></div><div class='meta'>score=14 · comments=7</div><div style='margin-top:10px'><a class='btn' href='https://www.reddit.com/r/MachineLearning/comments/1qrrayn/d_monthly_whos_hiring_and_who_wants_to_be_hired/'>打开链接</a></div></div>
<div class='card'><div class='row'><div style='display:flex;gap:8px;flex-wrap:wrap'><span class='pill' style='background:#111827'>Reddit</span><span class='pill' style='background:#374151'>r/MachineLearning</span></div></div><div class='title'>09. <a href='https://www.reddit.com/r/MachineLearning/comments/1r28sy7/r_iclr_guess_which_peer_review_is_human_or_ai/'>[R] ICLR: Guess which peer review is human or AI?</a></div><div class='meta'>score=26 · comments=17</div><div style='margin-top:10px'><a class='btn' href='https://www.reddit.com/r/MachineLearning/comments/1r28sy7/r_iclr_guess_which_peer_review_is_human_or_ai/'>打开链接</a></div></div>
<div class='card'><div class='row'><div style='display:flex;gap:8px;flex-wrap:wrap'><span class='pill' style='background:#111827'>Reddit</span><span class='pill' style='background:#374151'>r/MachineLearning</span></div></div><div class='title'>10. <a href='https://www.reddit.com/r/MachineLearning/comments/1r2gpz6/p_graph_representation_learning_help/'>[P] Graph Representation Learning Help</a></div><div class='meta'>score=4 · comments=3</div><div style='margin-top:10px'><a class='btn' href='https://www.reddit.com/r/MachineLearning/comments/1r2gpz6/p_graph_representation_learning_help/'>打开链接</a></div></div>
</div>
</div>
<div class='foot'>Archive: <a href='./archive.html'>历史归档</a></div>
</div></body></html>